from flask import Blueprint, request, jsonify
import pandas as pd
import numpy as np
from services.db_handler import get_db_connection
import re
import os
import pickle
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from gemini_handler import get_gemini_response
from datetime import datetime

chat4_bp = Blueprint("chat4", __name__)

# ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
MODEL_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'LLM_model'))
EXCEL_DATA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'excel_data'))

def load_pickle(filename):
    with open(os.path.join(MODEL_DIR, filename), "rb") as f:
        return pickle.load(f)

def save_pickle(data, filename):
    with open(os.path.join(MODEL_DIR, filename), "wb") as f:
        pickle.dump(data, f)

# ì„ë² ë”© ëª¨ë¸
embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# Excel/RAG í”„ë¡œì„¸ì„œ
class ExcelRAGProcessor:
    excel_path = os.path.join(os.path.dirname(__file__), "..", "services", "product_parts.xlsx")

    def __init__(self):
        self.documents = []
        self.embeddings = None
        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000)
        self.tfidf_matrix = None

    def load_excel_data_from_db(self):
        try:
            print("ğŸ”„ DBì—ì„œ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
            conn = get_db_connection()
            query = "SELECT * FROM pcb_parts"
            df = pd.read_sql(query, conn)
            conn.close()

            if df.empty:
                print("âŒ DBì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False

            print(f"âœ… DBì—ì„œ {len(df)}ê°œì˜ ë ˆì½”ë“œë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            df.to_excel(self.excel_path, index=False, sheet_name='pcb_parts')
            print(f"ğŸ’¾ Excel íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {self.excel_path}")
            return True

        except Exception as e:
            print(f"âŒ DBì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return False

    def load_excel_data(self, excel_path=None):
        try:
            if not self.load_excel_data_from_db():
                print("âš ï¸ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨, ê¸°ì¡´ Excel íŒŒì¼ ì‚¬ìš©")

            if not os.path.exists(self.excel_path):
                print(f"âŒ Excel íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.excel_path}")
                return False

            excel_file = pd.ExcelFile(self.excel_path)
            all_documents = []

            for sheet_name in excel_file.sheet_names:
                df = pd.read_excel(self.excel_path, sheet_name=sheet_name)

                for idx, row in df.iterrows():
                    row_text = " ".join([
                        f"{col}: {str(val)}"
                        for col, val in row.items()
                        if pd.notna(val)
                    ])
                    document = {
                        'content': row_text,
                        'sheet': sheet_name,
                        'row_index': idx,
                        'metadata': row.to_dict()
                    }
                    all_documents.append(document)

            self.documents = all_documents
            print(f"ğŸ“Š ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(self.documents)}")
            return True

        except Exception as e:
            print(f"âŒ EXCEL íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False

    def create_embeddings(self):
        if not self.documents:
            print("ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € EXCEL íŒŒì¼ì„ ë¡œë“œí•˜ì„¸ìš”.")
            return False
        try:
            texts = [doc['content'] for doc in self.documents]
            cleaned_texts = [self.clean_text(text) for text in texts]
            print("ì„ë² ë”© ìƒì„± ì¤‘...")
            self.embeddings = embedding_model.encode(cleaned_texts)
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(cleaned_texts)
            print(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {self.embeddings.shape}")
            return True
        except Exception as e:
            print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return False

    def clean_text(self, text):
        text = str(text).lower()
        # í•˜ì´í”ˆ(-) ë³´ì¡´: ë¶€í’ˆë²ˆí˜¸ ë§¤ì¹­ ì •í™•ë„ í–¥ìƒ
        text = re.sub(r"[^\w\sê°€-í£\-]", " ", text)
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    def search_documents(self, query, top_k=5, min_similarity=0.35):
        if self.embeddings is None:
            return []
        try:
            query_cleaned = self.clean_text(query)
            query_embedding = embedding_model.encode([query_cleaned])

            similarities = cosine_similarity(query_embedding, self.embeddings)[0]
            query_tfidf = self.tfidf_vectorizer.transform([query_cleaned])
            tfidf_similarities = (self.tfidf_matrix * query_tfidf.T).toarray().flatten()

            combined_scores = 0.7 * similarities + 0.3 * tfidf_similarities

            # ë¶€í’ˆë²ˆí˜¸ íŒ¨í„´ ê°ì§€ ì‹œ ë¶€ë¶„ì¼ì¹˜ ê°€ì¤‘ì¹˜ ë¶€ì—¬
            m = re.search(r'[A-Z0-9\-]{6,}', query.upper())
            if m:
                qpart = m.group(0)
                for i, doc in enumerate(self.documents):
                    pn = str(doc['metadata'].get('part_number', '')).upper()
                    if pn and qpart in pn:
                        combined_scores[i] += 0.3

            top_indices = np.argsort(combined_scores)[::-1][:top_k]
            results = []
            for idx in top_indices:
                if combined_scores[idx] >= min_similarity:
                    results.append({
                        'document': self.documents[idx],
                        'similarity': float(combined_scores[idx]),
                        'semantic_sim': float(similarities[idx]),
                        'tfidf_sim': float(tfidf_similarities[idx])
                    })
            return results
        except Exception as e:
            print(f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def save_processed_data(self):
        try:
            save_pickle(self.documents, "excel_documents.pkl")
            save_pickle(self.embeddings, "excel_embeddings.pkl")
            save_pickle(self.tfidf_vectorizer, "excel_tfidf_vectorizer.pkl")
            save_pickle(self.tfidf_matrix, "excel_tfidf_matrix.pkl")
            print("ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

    def load_processed_data(self):
        try:
            self.documents = load_pickle("excel_documents.pkl")
            self.embeddings = load_pickle("excel_embeddings.pkl")
            self.tfidf_vectorizer = load_pickle("excel_tfidf_vectorizer.pkl")
            self.tfidf_matrix = load_pickle("excel_tfidf_matrix.pkl")
            print(f"ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
            return True
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False

# ì „ì—­ RAG ì¸ìŠ¤í„´ìŠ¤
rag_processor = ExcelRAGProcessor()

def initialize_rag_system(excel_path=None):
    if excel_path is None:
        excel_path = os.path.join(os.path.dirname(__file__), "..", "services", "product_parts.xlsx")
    if rag_processor.load_processed_data():
        return True
    if excel_path and os.path.exists(excel_path):
        if rag_processor.load_excel_data(excel_path):
            if rag_processor.create_embeddings():
                rag_processor.save_processed_data()
                return True
    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
    return False

def generate_rag_response(query, search_results):
    if not search_results:
        return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    context_parts = []
    for i, result in enumerate(search_results[:3]):
        doc = result['document']
        similarity = result['similarity']
        context_parts.append(
            f"[ë¬¸ì„œ {i+1}] (ìœ ì‚¬ë„: {similarity:.3f})\n"
            f"ì‹œíŠ¸: {doc['sheet']}\n"
            f"ë‚´ìš©: {doc['content'][:500]}...\n"
        )
    context = "\n".join(context_parts)
    prompt = f"""
ë‹¤ìŒì€ EXCEL ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:

{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ì˜ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
ê²€ìƒ‰ëœ ë°ì´í„°ì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ë˜, ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
    return get_gemini_response(prompt, apply_format=True)

@chat4_bp.route("/chat4", methods=["POST"])
def chat4():
    data = request.get_json()
    messages = data.get("messages", [])
    user_input = next(
        (msg.get("content", "") for msg in reversed(messages) if msg.get("role") == "user"),
        ""
    )

    if not user_input:
        return jsonify({"message": {"role": "assistant", "content": "ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."}})

    try:
        print("ğŸ”„ ì±„íŒ… ìš”ì²­ - DBì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        if rag_processor.load_excel_data_from_db():
            if rag_processor.load_excel_data():
                if rag_processor.create_embeddings():
                    print("âœ… ìµœì‹  ë°ì´í„°ë¡œ ì„ë² ë”© ì¬ìƒì„± ì™„ë£Œ")
                else:
                    print("âš ï¸ ì„ë² ë”© ì¬ìƒì„± ì‹¤íŒ¨")

        search_results = rag_processor.search_documents(user_input, top_k=5, min_similarity=0.35)

        if search_results:
            response = generate_rag_response(user_input, search_results)
        else:
            response = get_gemini_response(user_input, apply_format=True)

        return jsonify({"message": {"role": "assistant", "content": response}})

    except Exception as e:
        return jsonify({"message": {"role": "assistant", "content": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}})

# ì¬ê³  ê´€ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ (ë¯¸ì‚¬ìš© ì„¤ëª…ìš©)
INVENTORY_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ PCB-Managerì˜ ì¬ê³  ê´€ë¦¬ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
... ìƒëµ ...
"""

def analyze_inventory_intent(user_message):
    message_lower = user_message.lower()
    if any(keyword in message_lower for keyword in ["ë¶€ì¡±", "ë¶€ì¡±í•œ", "low stock", "shortage", "ì—†ëŠ”", "ë–¨ì–´ì§„"]):
        return "low_stock"
    if any(keyword in message_lower for keyword in ["í¡ìŠµ", "moisture", "ìŠµë„", "humidity", "ê±´ì¡°", "ë³´ê´€"]):
        return "moisture_management"
    if any(keyword in message_lower for keyword in ["ë°œì£¼", "ì£¼ë¬¸", "order", "êµ¬ë§¤", "purchase", "ì‹ ì²­"]):
        return "ordering"
    if re.search(r'[A-Z]{2}[0-9]{2}[A-Z0-9\-]{6,}', user_message.upper()):
        return "part_search"
    if any(keyword in message_lower for keyword in ["ì‚¼ì„±", "samsung", "ë¬´ë¼íƒ€", "murata", "tdk", "kemet"]):
        return "manufacturer_search"
    if any(keyword in message_lower for keyword in ["í˜„í™©", "ìƒíƒœ", "status", "ì¬ê³ ëŸ‰", "ìˆ˜ëŸ‰", "í˜„ì¬"]):
        return "inventory_status"
    if any(keyword in message_lower for keyword in ["í†µê³„", "ë¶„ì„", "analysis", "statistics", "ì´", "ì „ì²´", "í‰ê· "]):
        return "statistics"
    return "general"

# ì •í™• ì¼ì¹˜ ìš°ì„  ì¡°íšŒ
def fetch_exact_part(part_no: str):
    try:
        conn = get_db_connection()
        cur = conn.cursor(dictionary=True)
        cur.execute("SELECT * FROM pcb_parts WHERE part_number=%s", (part_no,))
        row = cur.fetchone()
        cur.close()
        conn.close()
        return row
    except Exception as e:
        print("exact match ì¡°íšŒ ì˜¤ë¥˜:", e)
        return None

def generate_inventory_specific_response(user_message, search_results, intent):
    if not search_results:
        return f"""ğŸ“¦ ì¬ê³  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤

'{user_message}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
"""

    total_parts = len(search_results)
    context_parts = []

    for i, result in enumerate(search_results[:5]):
        doc = result['document']
        metadata = doc.get('metadata', {})
        similarity = result['similarity']

        # ì»¬ëŸ¼ ë§¤í•‘ ìˆ˜ì •
        part_id = metadata.get('part_id', metadata.get('partId', 'Unknown'))
        product_name = metadata.get('part_number', metadata.get('product_name', metadata.get('product', 'Unknown')))
        manufacturer = metadata.get('manufacturer', 'Unknown')
        quantity = int(metadata.get('quantity', 0) or 0)
        min_stock = int(metadata.get('min_stock', metadata.get('minimumStock', 0)) or 0)
        moisture_absorption = bool(metadata.get('is_humidity_sensitive', metadata.get('moisture_absorption', False)))

        if quantity < min_stock:
            stock_status = "ğŸ”´ ë¶€ì¡±"
            shortage = min_stock - quantity
        else:
            stock_status = "ğŸŸ¢ ì¶©ë¶„"
            shortage = 0

        moisture_status = "ğŸ’§ í¡ìŠµê´€ë¦¬í•„ìš”" if moisture_absorption else "ğŸŒ ì¼ë°˜ë³´ê´€"

        context_parts.append({
            'part_id': part_id,
            'product_name': product_name,
            'manufacturer': manufacturer,
            'quantity': quantity,
            'min_stock': min_stock,
            'stock_status': stock_status,
            'shortage': shortage,
            'moisture_status': moisture_status,
            'similarity': similarity
        })

    if intent == "part_search" and context_parts:
        p = context_parts[0]
        resp = f"""ğŸ” ë¶€í’ˆ ê²€ìƒ‰ ê²°ê³¼

**{p['product_name']}**
- ì œì¡°ì‚¬: {p['manufacturer']}
- í˜„ì¬ì¬ê³ : {p['quantity']}ê°œ (ìµœì†Œ: {p['min_stock']}ê°œ)
- ìƒíƒœ: {p['stock_status']}
- ë³´ê´€ì¡°ê±´: {p['moisture_status']}
- ê²€ìƒ‰ ì •í™•ë„: {p['similarity']:.1%}
"""
        if p['shortage'] > 0:
            resp += f"âš ï¸ ë°œì£¼ í•„ìš”: {p['shortage']}ê°œ ë¶€ì¡±\n"
        return resp

    # ê¸°íƒ€ ì˜ë„ ì²˜ë¦¬ ê°„ë‹¨ ë²„ì „
    summary = [f"{i+1}. {p['product_name']} - {p['quantity']}ê°œ ({p['stock_status']})"
               for i, p in enumerate(context_parts[:3])]
    return "ğŸ“Š ì¬ê³  ë¶„ì„ ê²°ê³¼\n\n" + "\n".join(summary)

@chat4_bp.route('/inventory-chat', methods=['POST'])
def inventory_chat():
    try:
        print("\n" + "="*60)
        print("[ğŸ“] ì¬ê³  ì±—ë´‡ API í˜¸ì¶œ ì‹œì‘")
        print("="*60)

        data = request.get_json()
        if not data:
            return jsonify({"error": "ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", "success": False}), 400

        user_message = data.get('message', '').strip()
        context = data.get('context', {})

        print(f"[ğŸ“‹] ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}")
        print(f"[ğŸ“‹] ìš”ì²­ ì‹œê°„: {datetime.now().isoformat()}")

        if not user_message:
            return jsonify({"error": "ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.", "success": False}), 400

        intent = analyze_inventory_intent(user_message)
        print(f"[ğŸ§ ] ë¶„ì„ëœ ì˜ë„: {intent}")

        # 1) ì •í™• ì¼ì¹˜ ìš°ì„  ë°˜í™˜
        exact = fetch_exact_part(user_message)
        if exact:
            moisture_status = "ğŸ’§ í¡ìŠµê´€ë¦¬í•„ìš”" if exact.get('is_humidity_sensitive') else "ğŸŒ ì¼ë°˜ë³´ê´€"
            return jsonify({
                "response": f"""ğŸ” ë¶€í’ˆ ê²€ìƒ‰ ê²°ê³¼

**{exact.get('part_number','-')}**
- ì œì¡°ì‚¬: {exact.get('manufacturer','-')}
- ì‚¬ì´ì¦ˆ: {exact.get('size','-')}
- ì…ê³ ì¼: {exact.get('received_date','-')}
- í˜„ì¬ì¬ê³ : {int(exact.get('quantity',0))}ê°œ (ìµœì†Œ: {int(exact.get('min_stock',0))}ê°œ)
- ë³´ê´€ì¡°ê±´: {moisture_status}
""",
                "intent": "part_search",
                "search_results_count": 1,
                "timestamp": datetime.now().isoformat(),
                "success": True
            })

        # 2) ìµœì‹  ë°ì´í„° ë¡œë“œ ë° ì„ë² ë”©
        print("[ğŸ“Š] DBì—ì„œ ìµœì‹  ì¬ê³  ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        if rag_processor.load_excel_data_from_db():
            if rag_processor.load_excel_data():
                if rag_processor.create_embeddings():
                    print("âœ… ìµœì‹  ë°ì´í„°ë¡œ ì„ë² ë”© ì¬ìƒì„± ì™„ë£Œ")
            else:
                print("âš ï¸ ì„ë² ë”© ì¬ìƒì„± ì‹¤íŒ¨")

        # 3) RAG ê²€ìƒ‰ (ë³´ìˆ˜ì  íŒŒë¼ë¯¸í„°)
        search_results = rag_processor.search_documents(user_message, top_k=5, min_similarity=0.35)
        print(f"[ğŸ”] ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ë¬¸ì„œ")

        if search_results:
            response = generate_inventory_specific_response(user_message, search_results, intent)
        else:
            response = f"""ğŸ“¦ ì¬ê³  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤

'{user_message}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
"""

        print(f"[âœ…] ì‘ë‹µ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(response)}ì)")

        result = {
            "response": response,
            "intent": intent,
            "search_results_count": len(search_results),
            "timestamp": datetime.now().isoformat(),
            "success": True,
            "performance": {
                "intent": intent,
                "documents_found": len(search_results)
            }
        }

        return jsonify(result)

    except Exception as e:
        print(f"[âŒ] ì¬ê³  ì±—ë´‡ API ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "error": f"ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "success": False,
            "timestamp": datetime.now().isoformat()
        }), 500


@chat4_bp.route('/quick_actions', methods=['POST', 'OPTIONS'])  # Alternative URL with underscore
def quick_actions():
    # Handle CORS preflight request
    if request.method == 'OPTIONS':
        return '', 200
    
    try:
        data = request.get_json()
        action = data.get('action', '')
        print(f"[âš¡] ë¹ ë¥¸ ì•¡ì…˜ ìš”ì²­: {action}")

        action_queries = {
            "low_stock": "ë¶€ì¡±í•œ ì¬ê³  ë¶€í’ˆ minimum stock shortage",
            "moisture_management": "í¡ìŠµ ê´€ë¦¬ í•„ìš” moisture absorption humidity sensitive",
            "ordering_recommendation": "ë°œì£¼ ì¶”ì²œ order recommendation low stock"
        }
        query = action_queries.get(action, action)

        if rag_processor.load_excel_data():
            rag_processor.create_embeddings()

        search_results = rag_processor.search_documents(query, top_k=10, min_similarity=0.3)
        response = generate_inventory_specific_response(query, search_results, action)

        return jsonify({
            "response": response,
            "action": action,
            "search_results_count": len(search_results),
            "success": True,
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        print(f"[âŒ] ë¹ ë¥¸ ì•¡ì…˜ ì˜¤ë¥˜: {e}")
        return jsonify({"error": str(e), "success": False}), 500

@chat4_bp.route('/health', methods=['GET'])
def inventory_health():
    try:
        has_documents = len(rag_processor.documents) > 0
        has_embeddings = rag_processor.embeddings is not None

        try:
            conn = get_db_connection()
            conn.close()
            db_status = "connected"
        except:
            db_status = "disconnected"

        return jsonify({
            "status": "healthy" if has_documents and has_embeddings and db_status == "connected" else "degraded",
            "message": "ì¬ê³  ê´€ë¦¬ ì±—ë´‡ì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.",
            "timestamp": datetime.now().isoformat(),
            "components": {
                "database": db_status,
                "rag_documents": f"{len(rag_processor.documents)}ê°œ ë¡œë“œë¨" if has_documents else "ë¬¸ì„œ ì—†ìŒ",
                "embeddings": "ì¤€ë¹„ë¨" if has_embeddings else "ì¤€ë¹„ ì•ˆë¨",
                "system": "operational"
            }
        })

    except Exception as e:
        return jsonify({
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

# ì•± ì‹œì‘ì‹œ ì´ˆê¸°í™”
initialize_rag_system()
