from flask import Blueprint, request, jsonify
import pandas as pd
import numpy as np
from services.db_handler import get_db_connection
import re
import os
import pickle
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from gemini_handler import get_gemini_response

chat3_bp = Blueprint("chat3", __name__)

# âœ… ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
MODEL_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'LLM_model'))
EXCEL_DATA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'excel_data'))

def load_pickle(filename):
    """í”¼í´ íŒŒì¼ ë¡œë“œ"""
    with open(os.path.join(MODEL_DIR, filename), "rb") as f:
        return pickle.load(f)

def save_pickle(data, filename):
    """í”¼í´ íŒŒì¼ ì €ì¥"""
    with open(os.path.join(MODEL_DIR, filename), "wb") as f:
        pickle.dump(data, f)

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# âœ… EXCEL ë°ì´í„° ì²˜ë¦¬ ë° ì„ë² ë”© ìˆ˜ì •ë¶€ë¶„
class ExcelRAGProcessor:
    excel_path = os.path.join(os.path.dirname(__file__), "..", "services", "product_parts.xlsx")

    def __init__(self):
        self.documents = []
        self.embeddings = None
        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000)
        self.tfidf_matrix = None
        
    def load_excel_data_from_db(self):
        """DBì—ì„œ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ Excel íŒŒì¼ ì—…ë°ì´íŠ¸"""
        try:
            print("ğŸ”„ DBì—ì„œ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
            
            # DB ì—°ê²°
            conn = get_db_connection()
            
            # DBì—ì„œ ëª¨ë“  ë°ì´í„° ì¡°íšŒ
            query = "SELECT * FROM pcb_parts"
            df = pd.read_sql(query, conn)
            conn.close()
            
            if df.empty:
                print("âŒ DBì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            print(f"âœ… DBì—ì„œ {len(df)}ê°œì˜ ë ˆì½”ë“œë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            
            # Excel íŒŒì¼ë¡œ ì €ì¥
            df.to_excel(self.excel_path, index=False, sheet_name='pcb_parts')
            print(f"ğŸ’¾ Excel íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {self.excel_path}")
            
            return True
            
        except Exception as e:
            print(f"âŒ DBì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return False
        
    def load_excel_data(self, excel_path=None):
        """EXCEL íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¡œ ë³€í™˜ (DBì—ì„œ ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ í›„)"""
        try:
            # ë¨¼ì € DBì—ì„œ ìµœì‹  ë°ì´í„°ë¡œ Excel íŒŒì¼ ì—…ë°ì´íŠ¸
            if not self.load_excel_data_from_db():
                print("âš ï¸ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨, ê¸°ì¡´ Excel íŒŒì¼ ì‚¬ìš©")
            
            # Excel íŒŒì¼ ì½ê¸°
            if not os.path.exists(self.excel_path):
                print(f"âŒ Excel íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.excel_path}")
                return False
                
            excel_file = pd.ExcelFile(self.excel_path)
            all_documents = []
            
            for sheet_name in excel_file.sheet_names:
                df = pd.read_excel(self.excel_path, sheet_name=sheet_name)
                
                # ê° í–‰ì„ ë¬¸ì„œë¡œ ë³€í™˜
                for idx, row in df.iterrows():
                    # NaN ê°’ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    row_text = " ".join([
                        f"{col}: {str(val)}" 
                        for col, val in row.items() 
                        if pd.notna(val)
                    ])
                    
                    document = {
                        'content': row_text,
                        'sheet': sheet_name,
                        'row_index': idx,
                        'metadata': row.to_dict()
                    }
                    all_documents.append(document)
            
            self.documents = all_documents
            print(f"ğŸ“Š ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(self.documents)}")
            return True
            
        except Exception as e:
            print(f"âŒ EXCEL íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def create_embeddings(self):
        """ë¬¸ì„œë“¤ì˜ ì„ë² ë”© ìƒì„±"""
        if not self.documents:
            print("ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € EXCEL íŒŒì¼ì„ ë¡œë“œí•˜ì„¸ìš”.")
            return False
        
        try:
            # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            texts = [doc['content'] for doc in self.documents]
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            cleaned_texts = [self.clean_text(text) for text in texts]
            
            # Sentence Transformer ì„ë² ë”© ìƒì„±
            print("ì„ë² ë”© ìƒì„± ì¤‘...")
            self.embeddings = embedding_model.encode(cleaned_texts)
            
            # TF-IDF ë²¡í„°í™”
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(cleaned_texts)
            
            print(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {self.embeddings.shape}")
            return True
            
        except Exception as e:
            print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return False
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        text = str(text).lower()
        text = re.sub(r"[^\w\sê°€-í£]", " ", text)  # í•œê¸€ ì§€ì›
        text = re.sub(r"\s+", " ", text)
        return text.strip()
    
    def search_documents(self, query, top_k=5, min_similarity=0.3):
        """ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
        if self.embeddings is None:
            return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_cleaned = self.clean_text(query)
            query_embedding = embedding_model.encode([query_cleaned])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(query_embedding, self.embeddings)[0]
            
            # TF-IDF ìœ ì‚¬ë„ë„ ê³„ì‚°
            query_tfidf = self.tfidf_vectorizer.transform([query_cleaned])
            tfidf_similarities = (self.tfidf_matrix * query_tfidf.T).toarray().flatten()
            
            # ë‘ ìœ ì‚¬ë„ ì ìˆ˜ ê²°í•© (ê°€ì¤‘í‰ê· )
            combined_scores = 0.7 * similarities + 0.3 * tfidf_similarities
            
            # ìƒìœ„ kê°œ ë¬¸ì„œ ì„ íƒ
            top_indices = np.argsort(combined_scores)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                if combined_scores[idx] >= min_similarity:
                    results.append({
                        'document': self.documents[idx],
                        'similarity': float(combined_scores[idx]),
                        'semantic_sim': float(similarities[idx]),
                        'tfidf_sim': float(tfidf_similarities[idx])
                    })
            
            return results
            
        except Exception as e:
            print(f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def save_processed_data(self):
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        try:
            save_pickle(self.documents, "excel_documents.pkl")
            save_pickle(self.embeddings, "excel_embeddings.pkl")
            save_pickle(self.tfidf_vectorizer, "excel_tfidf_vectorizer.pkl")
            save_pickle(self.tfidf_matrix, "excel_tfidf_matrix.pkl")
            print("ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def load_processed_data(self):
        """ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
        try:
            self.documents = load_pickle("excel_documents.pkl")
            self.embeddings = load_pickle("excel_embeddings.pkl")
            self.tfidf_vectorizer = load_pickle("excel_tfidf_vectorizer.pkl")
            self.tfidf_matrix = load_pickle("excel_tfidf_matrix.pkl")
            print(f"ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
            return True
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False

# âœ… ì „ì—­ RAG í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
rag_processor = ExcelRAGProcessor()

def initialize_rag_system(excel_path=None):
    if excel_path is None:
        excel_path = os.path.join(os.path.dirname(__file__), "..", "services", "product_parts.xlsx")
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    # ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
    if rag_processor.load_processed_data():
        return True
    
    # ìƒˆë¡œ EXCEL íŒŒì¼ ì²˜ë¦¬
    if excel_path and os.path.exists(excel_path):
        if rag_processor.load_excel_data(excel_path):
            if rag_processor.create_embeddings():
                rag_processor.save_processed_data()
                return True
    
    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
    return False

def generate_rag_response(query, search_results):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ RAG ì‘ë‹µ ìƒì„±"""
    if not search_results:
        return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    for i, result in enumerate(search_results[:3]):  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
        doc = result['document']
        similarity = result['similarity']
        
        context_parts.append(
            f"[ë¬¸ì„œ {i+1}] (ìœ ì‚¬ë„: {similarity:.3f})\n"
            f"ì‹œíŠ¸: {doc['sheet']}\n"
            f"ë‚´ìš©: {doc['content'][:500]}...\n"
        )
    
    context = "\n".join(context_parts)
    
    # Geminiì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
ë‹¤ìŒì€ EXCEL ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:

{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ì˜ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
ê²€ìƒ‰ëœ ë°ì´í„°ì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ë˜, ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
    
    return get_gemini_response(prompt, apply_format=True)

@chat3_bp.route("/chat", methods=["POST"])
def chat():
    """RAG ê¸°ë°˜ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (ë§¤ë²ˆ DBì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)"""
    data = request.get_json()
    messages = data.get("messages", [])
    user_input = next(
        (msg.get("content", "") for msg in reversed(messages) if msg.get("role") == "user"), 
        ""
    )

    if not user_input:
        return jsonify({
            "message": {"role": "assistant", "content": "ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."}
        })

    try:
        # ë§¤ë²ˆ DBì—ì„œ ìµœì‹  ë°ì´í„°ë¡œ Excel íŒŒì¼ ì—…ë°ì´íŠ¸
        print("ğŸ”„ ì±„íŒ… ìš”ì²­ - DBì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        if rag_processor.load_excel_data():
            # ì„ë² ë”© ì¬ìƒì„± (ìƒˆë¡œìš´ ë°ì´í„°ì— ë§ê²Œ)
            if rag_processor.create_embeddings():
                print("âœ… ìµœì‹  ë°ì´í„°ë¡œ ì„ë² ë”© ì¬ìƒì„± ì™„ë£Œ")
            else:
                print("âš ï¸ ì„ë² ë”© ì¬ìƒì„± ì‹¤íŒ¨")
        
        # RAG ê²€ìƒ‰ ìˆ˜í–‰
        search_results = rag_processor.search_documents(user_input, top_k=5)
        
        if search_results:
            # ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
            response = generate_rag_response(user_input, search_results)
            
            final_response = response
        else:
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ì¼ë°˜ ì‘ë‹µ
            final_response = get_gemini_response(user_input, apply_format=True)
        
        return jsonify({
            "message": {"role": "assistant", "content": final_response}
        })
        
    except Exception as e:
        return jsonify({
            "message": {"role": "assistant", "content": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
        })

# ì¬ê³  ê´€ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
INVENTORY_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ PCB-Managerì˜ ì¬ê³  ê´€ë¦¬ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì£¼ìš” ì—­í• :**
1. ë¶€í’ˆ ì¬ê³  í˜„í™© ì¡°íšŒ ë° ë¶„ì„
2. ì¬ê³  ë¶€ì¡± ì•Œë¦¼ ë° ë°œì£¼ ì œì•ˆ
3. í¡ìŠµ ê´€ë¦¬ê°€ í•„ìš”í•œ ë¶€í’ˆ ì‹ë³„
4. ë¶€í’ˆ ìƒì„¸ ì •ë³´ ì œê³µ
5. ì¬ê³  ìµœì í™” ì¡°ì–¸

**ì‘ë‹µ ê·œì¹™:**
- ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ë‹µë³€
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„° í¬í•¨
- ì¬ê³  ê´€ë¦¬ì— ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ì •ë³´ ì œê³µ
- í•„ìš”ì‹œ ë°œì£¼ ì œì•ˆì´ë‚˜ ì£¼ì˜ì‚¬í•­ ì•ˆë‚´
- ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ

**ì¤‘ìš” ì •ë³´:**
- ì¬ê³ ëŸ‰ì´ ìµœì†Œ ì¬ê³ ëŸ‰ë³´ë‹¤ ë‚®ìœ¼ë©´ 'ë¶€ì¡±' ìƒíƒœ
- í¡ìŠµ ê´€ë¦¬ê°€ í•„ìš”í•œ ë¶€í’ˆì€ ë³„ë„ ë³´ê´€ í•„ìš”
- ë¶€í’ˆ IDëŠ” ì •í™•í•œ ë§¤ì¹­ì´ ì¤‘ìš”í•¨
"""

def analyze_inventory_intent(user_message):
    """ì‚¬ìš©ì ì˜ë„ ë¶„ì„ (ì¬ê³  ê´€ë¦¬ íŠ¹í™”)"""
    message_lower = user_message.lower()
    
    # ë¶€ì¡± ì¬ê³  ê´€ë ¨
    if any(keyword in message_lower for keyword in ["ë¶€ì¡±", "ë¶€ì¡±í•œ", "low stock", "shortage", "ì—†ëŠ”", "ë–¨ì–´ì§„"]):
        return "low_stock"
    
    # í¡ìŠµ ê´€ë¦¬ ê´€ë ¨
    if any(keyword in message_lower for keyword in ["í¡ìŠµ", "moisture", "ìŠµë„", "humidity", "ê±´ì¡°", "ë³´ê´€"]):
        return "moisture_management"
    
    # ë°œì£¼ ê´€ë ¨
    if any(keyword in message_lower for keyword in ["ë°œì£¼", "ì£¼ë¬¸", "order", "êµ¬ë§¤", "purchase", "ì‹ ì²­"]):
        return "ordering"
    
    # íŠ¹ì • ë¶€í’ˆ ê²€ìƒ‰
    if re.search(r'[A-Z]{2}[0-9]{2}[A-Z0-9]{6,}', user_message.upper()):
        return "part_search"
    
    # ì œì¡°ì‚¬ ê²€ìƒ‰
    if any(keyword in message_lower for keyword in ["ì‚¼ì„±", "samsung", "ë¬´ë¼íƒ€", "murata", "tdk", "kemet"]):
        return "manufacturer_search"
    
    # ì¬ê³  í˜„í™©
    if any(keyword in message_lower for keyword in ["í˜„í™©", "ìƒíƒœ", "status", "ì¬ê³ ëŸ‰", "ìˆ˜ëŸ‰", "í˜„ì¬"]):
        return "inventory_status"
    
    # í†µê³„ ë° ë¶„ì„
    if any(keyword in message_lower for keyword in ["í†µê³„", "ë¶„ì„", "analysis", "statistics", "ì´", "ì „ì²´", "í‰ê· "]):
        return "statistics"
    
    return "general"

def generate_inventory_specific_response(user_message, search_results, intent):
    """ì¬ê³  ê´€ë¦¬ì— íŠ¹í™”ëœ ì‘ë‹µ ìƒì„±"""
    if not search_results:
        return f"""ğŸ“¦ **ì¬ê³  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤**

'{user_message}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ğŸ’¡ **ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”:**
- ì •í™•í•œ ë¶€í’ˆ ID ì…ë ¥ (ì˜ˆ: CL02B121KP2NNNC)
- ì œì¡°ì‚¬ ì´ë¦„ í™•ì¸ (ì‚¼ì„±, ë¬´ë¼íƒ€ ë“±)
- "ì¬ê³  í˜„í™©", "ë¶€ì¡±í•œ ë¶€í’ˆ" ë“±ìœ¼ë¡œ ì§ˆë¬¸

ğŸ” **ì§€ì› ê°€ëŠ¥í•œ ì§ˆë¬¸:**
- "ë¶€ì¡±í•œ ì¬ê³  ì•Œë ¤ì¤˜"
- "í¡ìŠµ ê´€ë¦¬ í•„ìš”í•œ ë¶€í’ˆ"
- "ì‚¼ì„± ì»¤íŒ¨ì‹œí„° í˜„í™©"
- "ì „ì²´ ì¬ê³  í†µê³„"
"""
    
    # ê²€ìƒ‰ëœ ë°ì´í„° ë¶„ì„
    total_parts = len(search_results)
    context_parts = []
    
    for i, result in enumerate(search_results[:5]):  # ìƒìœ„ 5ê°œ
        doc = result['document']
        metadata = doc.get('metadata', {})
        similarity = result['similarity']
        
        # ë¶€í’ˆ ì •ë³´ ì¶”ì¶œ
        part_id = metadata.get('part_id', metadata.get('partId', 'Unknown'))
        product_name = metadata.get('product_name', metadata.get('product', 'Unknown'))
        manufacturer = metadata.get('manufacturer', 'Unknown')
        quantity = metadata.get('quantity', 0)
        min_stock = metadata.get('min_stock', metadata.get('minimumStock', 0))
        moisture_absorption = metadata.get('moisture_absorption', metadata.get('moistureAbsorption', False))
        
        # ì¬ê³  ìƒíƒœ íŒë‹¨
        if quantity < min_stock:
            stock_status = "ğŸ”´ ë¶€ì¡±"
            shortage = min_stock - quantity
        else:
            stock_status = "ğŸŸ¢ ì¶©ë¶„"
            shortage = 0
        
        moisture_status = "ğŸ’§ í¡ìŠµê´€ë¦¬í•„ìš”" if moisture_absorption else "ğŸŒ ì¼ë°˜ë³´ê´€"
        
        context_parts.append({
            'part_id': part_id,
            'product_name': product_name,
            'manufacturer': manufacturer,
            'quantity': quantity,
            'min_stock': min_stock,
            'stock_status': stock_status,
            'shortage': shortage,
            'moisture_status': moisture_status,
            'similarity': similarity
        })
    
    # ì˜ë„ë³„ íŠ¹í™” ì‘ë‹µ ìƒì„±
    if intent == "low_stock":
        low_stock_parts = [p for p in context_parts if "ë¶€ì¡±" in p['stock_status']]
        
        response = f"ğŸ“¦ **ì¬ê³  ë¶€ì¡± í˜„í™© ë¶„ì„**\n\n"
        response += f"ğŸ” **ê²€ìƒ‰ëœ ë¶€í’ˆ**: {total_parts}ê°œ\n"
        response += f"ğŸ”´ **ë¶€ì¡± ë¶€í’ˆ**: {len(low_stock_parts)}ê°œ\n\n"
        
        if low_stock_parts:
            response += "**ì¦‰ì‹œ ë°œì£¼ê°€ í•„ìš”í•œ ë¶€í’ˆë“¤:**\n"
            for part in low_stock_parts[:3]:
                response += f"""
â€¢ **{part['part_id']}** ({part['product_name']})
  - í˜„ì¬: {part['quantity']}ê°œ | ìµœì†Œ: {part['min_stock']}ê°œ
  - ë¶€ì¡±ëŸ‰: {part['shortage']}ê°œ | ì œì¡°ì‚¬: {part['manufacturer']}
  - {part['moisture_status']}
"""
        else:
            response += "âœ… **ì–‘í˜¸**: ê²€ìƒ‰ëœ ë¶€í’ˆë“¤ì˜ ì¬ê³ ê°€ ì¶©ë¶„í•©ë‹ˆë‹¤!"
    
    elif intent == "moisture_management":
        moisture_parts = [p for p in context_parts if "í¡ìŠµê´€ë¦¬í•„ìš”" in p['moisture_status']]
        
        response = f"ğŸ’§ **í¡ìŠµ ê´€ë¦¬ í˜„í™©**\n\n"
        response += f"ğŸ” **ê²€ìƒ‰ëœ ë¶€í’ˆ**: {total_parts}ê°œ\n"
        response += f"ğŸ’§ **í¡ìŠµ ê´€ë¦¬ ëŒ€ìƒ**: {len(moisture_parts)}ê°œ\n\n"
        
        if moisture_parts:
            response += "**í¡ìŠµ ê´€ë¦¬ê°€ í•„ìš”í•œ ë¶€í’ˆ:**\n"
            for part in moisture_parts:
                response += f"""
â€¢ **{part['part_id']}** ({part['product_name']})
  - í˜„ì¬ ì¬ê³ : {part['quantity']}ê°œ | {part['stock_status']}
  - ì œì¡°ì‚¬: {part['manufacturer']}
  - âš ï¸ ìŠµë„ ê´€ë¦¬ í•„ìˆ˜
"""
            
            response += f"""
ğŸ“‹ **í¡ìŠµ ê´€ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- âœ… ê±´ì¡°ì œì™€ í•¨ê»˜ ë°€í ë³´ê´€
- âœ… ìŠµë„ 30% ì´í•˜ ìœ ì§€
- âœ… ê°œë´‰ í›„ 8ì‹œê°„ ë‚´ ì‚¬ìš©
- âœ… ì¬ê±´ì¡° ì£¼ê¸° ì¤€ìˆ˜

ğŸš¨ **ì£¼ì˜ì‚¬í•­:** í¡ìŠµëœ ë¶€í’ˆì€ PCB ë¶ˆëŸ‰ì˜ ì£¼ìš” ì›ì¸ì´ ë©ë‹ˆë‹¤.
"""
    
    elif intent == "part_search":
        if context_parts:
            part = context_parts[0]  # ê°€ì¥ ìœ ì‚¬í•œ ë¶€í’ˆ
            response = f"""ğŸ” **ë¶€í’ˆ ê²€ìƒ‰ ê²°ê³¼**

**{part['part_id']}**
- ì œí’ˆëª…: {part['product_name']}
- ì œì¡°ì‚¬: {part['manufacturer']}
- í˜„ì¬ì¬ê³ : {part['quantity']}ê°œ (ìµœì†Œ: {part['min_stock']}ê°œ)
- ìƒíƒœ: {part['stock_status']}
- ë³´ê´€ì¡°ê±´: {part['moisture_status']}
- ê²€ìƒ‰ ì •í™•ë„: {part['similarity']:.1%}

"""
            
            if part['shortage'] > 0:
                response += f"âš ï¸ **ë°œì£¼ í•„ìš”**: {part['shortage']}ê°œ ë¶€ì¡±\n"
            
            # ìœ ì‚¬í•œ ë‹¤ë¥¸ ë¶€í’ˆë“¤ë„ í‘œì‹œ
            if len(context_parts) > 1:
                response += f"\nğŸ” **ìœ ì‚¬í•œ ë¶€í’ˆë“¤:**\n"
                for similar_part in context_parts[1:3]:
                    response += f"- {similar_part['part_id']} ({similar_part['manufacturer']}) - {similar_part['quantity']}ê°œ\n"
    
    else:  # ì¼ë°˜ ì‘ë‹µ
        response = f"""ğŸ“Š **ì¬ê³  ë¶„ì„ ê²°ê³¼**

ğŸ” **ê²€ìƒ‰ëœ ë¶€í’ˆ**: {total_parts}ê°œ

**ìƒìœ„ ë¶€í’ˆ í˜„í™©:**
"""
        
        for i, part in enumerate(context_parts[:3], 1):
            response += f"""
{i}. **{part['part_id']}** ({part['product_name']})
   - ì¬ê³ : {part['quantity']}ê°œ | {part['stock_status']}
   - ì œì¡°ì‚¬: {part['manufacturer']} | {part['moisture_status']}
"""
        
        low_stock_count = len([p for p in context_parts if "ë¶€ì¡±" in p['stock_status']])
        moisture_count = len([p for p in context_parts if "í¡ìŠµê´€ë¦¬í•„ìš”" in p['moisture_status']])
        
        response += f"""
ğŸ“ˆ **ìš”ì•½:**
- ğŸ”´ ë¶€ì¡± ë¶€í’ˆ: {low_stock_count}ê°œ
- ğŸ’§ í¡ìŠµ ê´€ë¦¬: {moisture_count}ê°œ
- ğŸŸ¢ ì •ìƒ ì¬ê³ : {total_parts - low_stock_count}ê°œ
"""
    
    return response

@chat3_bp.route('/inventory-chat', methods=['POST'])
def inventory_chat():
    """ì¬ê³  ê´€ë¦¬ ì „ìš© ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸ (RAG ê¸°ë°˜)"""
    try:
        print("\n" + "="*60)
        print("[ğŸ“] ì¬ê³  ì±—ë´‡ API í˜¸ì¶œ ì‹œì‘")
        print("="*60)
        
        data = request.get_json()
        if not data:
            return jsonify({"error": "ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", "success": False}), 400
        
        user_message = data.get('message', '').strip()
        context = data.get('context', {})
        
        print(f"[ğŸ“‹] ì‚¬ìš©ì ë©”ì‹œì§€: {user_message}")
        print(f"[ğŸ“‹] ìš”ì²­ ì‹œê°„: {datetime.now().isoformat()}")
        
        if not user_message:
            return jsonify({"error": "ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.", "success": False}), 400
        
        # ì‚¬ìš©ì ì˜ë„ ë¶„ì„
        intent = analyze_inventory_intent(user_message)
        print(f"[ğŸ§ ] ë¶„ì„ëœ ì˜ë„: {intent}")
        
        # DBì—ì„œ ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
        print("[ğŸ“Š] DBì—ì„œ ìµœì‹  ì¬ê³  ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        if rag_processor.load_excel_data():
            if rag_processor.create_embeddings():
                print("âœ… ìµœì‹  ë°ì´í„°ë¡œ ì„ë² ë”© ì¬ìƒì„± ì™„ë£Œ")
            else:
                print("âš ï¸ ì„ë² ë”© ì¬ìƒì„± ì‹¤íŒ¨")
        
        # RAG ê²€ìƒ‰ ìˆ˜í–‰
        search_results = rag_processor.search_documents(user_message, top_k=10, min_similarity=0.1)
        
        print(f"[ğŸ”] ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ë¬¸ì„œ")
        
        # ì¬ê³  ê´€ë¦¬ íŠ¹í™” ì‘ë‹µ ìƒì„±
        if search_results:
            # ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ íŠ¹í™” ì‘ë‹µ
            response = generate_inventory_specific_response(user_message, search_results, intent)
        else:
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ì‘ë‹µ
            response = f"""ğŸ“¦ **ì¬ê³  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤**

'{user_message}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ğŸ’¡ **ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”:**
- ì •í™•í•œ ë¶€í’ˆ ID ì…ë ¥
- ì œì¡°ì‚¬ ì´ë¦„ í™•ì¸  
- "ì¬ê³  í˜„í™©", "ë¶€ì¡±í•œ ë¶€í’ˆ" ë“±ìœ¼ë¡œ ì§ˆë¬¸

ğŸ”§ **ì§€ì› ê°€ëŠ¥í•œ ê¸°ëŠ¥:**
- ë¶€í’ˆ ì¬ê³  í™•ì¸
- ì¬ê³  ë¶€ì¡± ì•Œë¦¼
- í¡ìŠµ ê´€ë¦¬ ë¶€í’ˆ ì¡°íšŒ
- ë°œì£¼ ì¶”ì²œ
"""
        
        print(f"[âœ…] ì‘ë‹µ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(response)}ì)")
        
        result = {
            "response": response,
            "intent": intent,
            "search_results_count": len(search_results),
            "timestamp": datetime.now().isoformat(),
            "success": True,
            "performance": {
                "intent": intent,
                "documents_found": len(search_results)
            }
        }
        
        return jsonify(result)
        
    except Exception as e:
        print(f"[âŒ] ì¬ê³  ì±—ë´‡ API ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "error": f"ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "success": False,
            "timestamp": datetime.now().isoformat()
        }), 500

@chat3_bp.route('/quick-actions', methods=['POST'])
def quick_actions():
    """ë¹ ë¥¸ ì•¡ì…˜ ì²˜ë¦¬ (ë¶€ì¡±ì¬ê³ , í¡ìŠµê´€ë¦¬, ë°œì£¼ì¶”ì²œ ë“±)"""
    try:
        data = request.get_json()
        action = data.get('action', '')
        
        print(f"[âš¡] ë¹ ë¥¸ ì•¡ì…˜ ìš”ì²­: {action}")
        
        # ì•¡ì…˜ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ë§¤í•‘
        action_queries = {
            "low_stock": "ë¶€ì¡±í•œ ì¬ê³  ë¶€í’ˆ minimum stock shortage",
            "moisture_management": "í¡ìŠµ ê´€ë¦¬ í•„ìš” moisture absorption humidity sensitive",
            "ordering_recommendation": "ë°œì£¼ ì¶”ì²œ order recommendation low stock"
        }
        
        query = action_queries.get(action, action)
        
        # DBì—ì„œ ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
        if rag_processor.load_excel_data():
            rag_processor.create_embeddings()
        
        # RAG ê²€ìƒ‰ ìˆ˜í–‰
        search_results = rag_processor.search_documents(query, top_k=20, min_similarity=0.05)
        intent = action.replace('_', ' ')
        
        # íŠ¹í™” ì‘ë‹µ ìƒì„±
        response = generate_inventory_specific_response(query, search_results, action)
        
        return jsonify({
            "response": response,
            "action": action,
            "search_results_count": len(search_results),
            "success": True,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"[âŒ] ë¹ ë¥¸ ì•¡ì…˜ ì˜¤ë¥˜: {e}")
        return jsonify({
            "error": str(e),
            "success": False
        }), 500

@chat3_bp.route('/health', methods=['GET'])
def inventory_health():
    """ì¬ê³  ì±—ë´‡ í—¬ìŠ¤ ì²´í¬"""
    try:
        # RAG ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        has_documents = len(rag_processor.documents) > 0
        has_embeddings = rag_processor.embeddings is not None
        
        # DB ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            conn = get_db_connection()
            conn.close()
            db_status = "connected"
        except:
            db_status = "disconnected"
        
        return jsonify({
            "status": "healthy" if has_documents and has_embeddings and db_status == "connected" else "degraded",
            "message": "ì¬ê³  ê´€ë¦¬ ì±—ë´‡ì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.",
            "timestamp": datetime.now().isoformat(),
            "components": {
                "database": db_status,
                "rag_documents": f"{len(rag_processor.documents)}ê°œ ë¡œë“œë¨" if has_documents else "ë¬¸ì„œ ì—†ìŒ",
                "embeddings": "ì¤€ë¹„ë¨" if has_embeddings else "ì¤€ë¹„ ì•ˆë¨",
                "system": "operational"
            }
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

#âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì•± ì‹œì‘ì‹œ í˜¸ì¶œ)
from datetime import datetime
initialize_rag_system()